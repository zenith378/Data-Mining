# -*- coding: utf-8 -*-
"""07_support_vector_machines.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12AdO2azS2VefFFT_5T0tE0tGgehFGzzX
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.svm import SVC

from collections import defaultdict

"""# Data Preparation"""

# we use the dataset without duplicated columns

X_test_df = pd.read_csv('best_X_test_con_nomi.csv', header = 0)
y_train_df = pd.read_csv('y_train_wo_outliers_num.csv', header = None)
X_train_df = pd.read_csv('best_X_train_con_nomi.csv', header = 0)
y_test_df = pd.read_fwf('y_test.txt', header = None)

X_test = X_test_df.values
y_train = y_train_df.values
X_train = X_train_df.values
y_test = y_test_df.values

"""remove class 5"""

X_train_n=np.zeros(shape=[0,23])
y_train_n=np.zeros(shape=[0,1])
for j in range(0,7274):
  if y_train[j]!=6:

    X_train_n = np.append(X_train_n, [X_train[j]], axis=0)
    y_train_n = np.append(y_train_n, [y_train[j]], axis=0)

X_test_n=np.zeros(shape=[0,23])
y_test_n=np.zeros(shape=[0,1])
for j in range(0,len(y_test)):
  if y_test[j]!=6:

    X_test_n = np.append(X_test_n, [X_test[j]], axis=0)
    y_test_n = np.append(y_test_n, [y_test[j]], axis=0)

X_test=X_test_n
y_test=y_test_n
X_train=X_train_n
y_train=y_train_n

from sklearn.model_selection import train_test_split, cross_val_score , GridSearchCV

from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.metrics import roc_curve, auc, roc_auc_score, plot_confusion_matrix

from sklearn.preprocessing import StandardScaler

"""normalize"""

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

"""#SVM

Support Vector Machine with default parameters
"""

clf_svm = SVC(random_state=42)
clf_svm.fit(X_train, y_train)

"""Confusion matrix to understand how good the calssifier works."""

plot_confusion_matrix(clf_svm,
                      X_test,
                      y_test,
                      cmap='Blues',
                      values_format='d',
                      colorbar=False
                      )
plt.savefig('confusion_matrix.pdf',bbox_inches = 'tight')

"""In this case, the classifier works pretty well, in particular for the class 3. The highest misclassification is between classes 4 and 5, and 1 and 2 (and 3).  """

y_pred = clf_svm.predict(X_test)

print('Accuracy %s' % accuracy_score(y_test, y_pred))
print('F1-score %s' % f1_score(y_test, y_pred, average=None))
print(classification_report(y_test, y_pred))

"""To do better we perform a grid search for the parameters. The search for the kernel is not done because the other ones are basically shit and takes too much time."""

param_grid=[
            {'C': [0.001,0.01,0.1,1,10,100],
             'gamma': ['auto','scale',1,0.1,0.01,0.001,0.0001],
             'kernel': ['rbf']},
                
]

optimal_params = GridSearchCV(
    SVC(),
    param_grid,
    cv=5,
    scoring='accuracy',
    verbose=0
)

optimal_params.fit(X_train,y_train)
print(optimal_params.best_params_)

target_names=['WALKING','WALKING UPSTAIRS','WALKING DOWNSTAIRS','SITTING','STANDING']

"""new analysis with optimal hyperparameters"""

clf_svm = SVC(random_state=42,C=10,gamma=0.01)
clf_svm.fit(X_train, y_train)

plot_confusion_matrix(clf_svm,
                      X_test,
                      y_test,
                      cmap='Blues',
                      values_format='d',
                      colorbar=False
                      )
plt.savefig('confusion_matrix_svm.pdf',bbox_inches = 'tight')

y_pred = clf_svm.predict(X_test)

print('Accuracy %s' % accuracy_score(y_test, y_pred))
print('F1-score %s' % f1_score(y_test, y_pred, average=None))

report=classification_report(y_test, y_pred,output_dict=True,digits=2,target_names=target_names)
report_df = pd.DataFrame(report).transpose()

print(report_df.to_latex())

"""# PCA"""

from sklearn.decomposition import PCA

pca = PCA()
X_train_pca = pca.fit_transform(X_train)

per_var = np.round(pca.explained_variance_ratio_ *100, decimals=1)
labels = [str(x) for x in range(1, len(per_var)+1)]

plt.bar(x=range(1,len(per_var)+1), height=per_var)
plt.tick_params(
    axis='x',
    which='both',
    bottom=False,
    top=False,
    labelbottom=False)
plt.ylabel('Percentage of Explained Variance')
plt.xlabel('Principal Components')
plt.show()

"""To visualize it, we perform a PCA. How many components? Better 3, because the jump between 3 and 4 is higher than the one between 2 and 3."""

pca = PCA(n_components=3)
pca.fit(X_train)
X_pca = pca.transform(X_train)

fig = plt.figure()
ax = fig.add_subplot(projection='3d')

ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y_train,cmap=plt.cm.prism, edgecolor='k', alpha=0.5)

ax.set_xlabel('1st PC')
ax.set_ylabel('2nd PC')
ax.set_zlabel('3rd PC')
plt.savefig('pca_svm.pdf', bbox_inches = 'tight')
plt.show()

"""Qua sotto ti faccio pure l'analisi a due componenti. Così, perché mi va."""

pca = PCA(n_components=2)
pca.fit(X_train)
X_pca = pca.transform(X_train)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap=plt.cm.prism, edgecolor='k', alpha=0.7)
plt.show()

"""# Vectors

PCA e vettori
"""

pca = PCA(n_components=2)
pca.fit(X_train)
X_pca = pca.transform(X_train)

decision_function = clf_svm.decision_function(X_train)
support_vector_indices = np.where((2 * y_train - 1) * decision_function <= 1)[0]
support_vectors = X_train[support_vector_indices]

support_vectors_pca = pca.transform(support_vectors)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap=plt.cm.prism, edgecolor='none', alpha=0.7, s=30)
plt.scatter(support_vectors_pca[:100, 0], support_vectors_pca[:100, 1], s=100,
                linewidth=1, facecolors='none', edgecolors='k')

plt.savefig('vectors.pdf', bbox_inches = 'tight')

plt.show()

"""# Nonlinear SVM"""

from sklearn.svm import LinearSVC

from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler

clf = SVC(gamma='auto')
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('Accuracy %s' % accuracy_score(y_test, y_pred))
print('F1-score %s' % f1_score(y_test, y_pred, average=None))
print(classification_report(y_test, y_pred))

clf = SVC(gamma='auto', C=0.1, kernel='rbf', random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print('Accuracy %s' % accuracy_score(y_test, y_pred))
print('F1-score %s' % f1_score(y_test, y_pred, average=None))
print(classification_report(y_test, y_pred))

clf.support_

clf.support_vectors_[:10]

clf.n_support_, len(X_train)