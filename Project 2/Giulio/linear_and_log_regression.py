# -*- coding: utf-8 -*-
"""Linear and log_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ruuOm9qQRUSHDKOGoF7Nz4n-XE6Q5orR

**Author:** Giulio Cordova
**Python version:**  3.x
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from collections import defaultdict

from sklearn.model_selection import cross_val_score 

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, PrecisionRecallDisplay, RocCurveDisplay, accuracy_score

"""#Load Data"""

# we use the dataset without duplicated columns

X_test_df = pd.read_csv('best_X_test_con_nomi.csv', header = 0)
y_train_df = pd.read_csv('y_train_wo_outliers_num.csv', header = None)
X_train_df = pd.read_csv('best_X_train_con_nomi.csv', header = 0)
y_test_df = pd.read_fwf('y_test.txt', header = None)

X_test = X_test_df.values
y_train = y_train_df.values
X_train = X_train_df.values
y_test = y_test_df.values

"""Remove class 5"""

X_train_n=np.zeros(shape=[0,23])
y_train_n=np.zeros(shape=[0,1])
for j in range(0,7274):
  if y_train[j]!=6:

    X_train_n = np.append(X_train_n, [X_train[j]], axis=0)
    y_train_n = np.append(y_train_n, [y_train[j]], axis=0)

X_test_n=np.zeros(shape=[0,23])
y_test_n=np.zeros(shape=[0,1])
for j in range(0,len(y_test)):
  if y_test[j]!=6:

    X_test_n = np.append(X_test_n, [X_test[j]], axis=0)
    y_test_n = np.append(y_test_n, [y_test[j]], axis=0)

X_test=X_test_n
y_test=y_test_n
X_train=X_train_n
y_train=y_train_n

"""# Linear Regression"""

from sklearn.linear_model import LinearRegression, Ridge, Lasso

reg = LinearRegression()
reg.fit(X_train, y_train)

print('Coefficients: \n', reg.coef_)
print('Intercept: \n', reg.intercept_)

y_pred = reg.predict(X_test)

print('R2: %.3f' % r2_score(y_test, y_pred))
print('MSE: %.3f' % mean_squared_error(y_test, y_pred))
print('MAE: %.3f' % mean_absolute_error(y_test, y_pred))

"""Try to divide the dataset according to class and predict class. We expect all coefficients to be zero and an intercept at the height of the class."""

X_train_class=[]
X_test_class=[]
y_train_class=[]
y_test_class=[]

for i in range(1,6):
  X_dummy=[]
  y_dummy=[]
  for j in range(0,len(y_train)):
    if y_train[j]==i:
      X_dummy.append(X_train[j])
      y_dummy.append(y_train[j])
  X_train_class.append(X_dummy)
  y_train_class.append(y_dummy)

for i in range(1,6):
  X_dummy=[]
  y_dummy=[]
  for j in range(0,len(y_test)):
    if y_train[j]==i:
      X_dummy.append(X_test[j])
      y_dummy.append(y_test[j])
  X_test_class.append(X_dummy)
  y_test_class.append(y_dummy)

for i in range(0,5):
  reg = LinearRegression()
  reg.fit(X_train_class[i], y_train_class[i])
  print('Multi regression for class ', i)
  print('Coefficients: \n', reg.coef_)
  print('Intercept: \n', reg.intercept_)
  y_pred = reg.predict(X_test_class[i])
  print('R2: %.3f' % r2_score(y_test_class[i], y_pred))
  print('MSE: %.3f' % mean_squared_error(y_test_class[i], y_pred))
  print('MAE: %.3f' % mean_absolute_error(y_test_class[i], y_pred))

"""Perform a linear regression over the 23 variables. Keep one as target and let the others be the support."""

col_names=list(X_train_df.columns)

coefficients=np.zeros(shape=(0,22))

coeff_r2=[]

for element in col_names:
  dummy_x_tr=X_train_df.loc[:, X_train_df.columns != element]
  dummy_y_tr=X_train_df.loc[:, X_train_df.columns == element]
  dummy_x_te=X_test_df.loc[:, X_test_df.columns != element]
  dummy_y_te=X_test_df.loc[:, X_test_df.columns == element]
  X_test_dummy = dummy_x_te.values
  y_train_dummy = dummy_y_tr.values
  X_train_dummy = dummy_x_tr.values
  y_test_dummy = dummy_y_te.values
  reg = LinearRegression()
  reg.fit(X_train_dummy, y_train_dummy)
  print('Multi regression for class ', element)
  print('Coefficients: \n', reg.coef_)
  print('Intercept: \n', reg.intercept_)

  dum_coef=reg.coef_
  #np.insert(dum_coef,0,1,axis=1)
  
  coefficients = np.append(coefficients, dum_coef, axis=0)
  #print(dum_coef)
  #dum_coef.insert(col_names.index(element),1)
  #print(dum_coef)
  #coefficients.append(dum_coef)

  y_pred = reg.predict(X_test_dummy)
  print('R2: %.3f' % r2_score(y_test_dummy, y_pred))
  print('MSE: %.3f' % mean_squared_error(y_test_dummy, y_pred))
  print('MAE: %.3f' % mean_absolute_error(y_test_dummy, y_pred))

  coeff_r2.append(r2_score(y_test_dummy, y_pred))

"""adding the diagonal 0 to the list of coefficients (b)"""

b = np.zeros((coefficients.shape[0], coefficients.shape[1]+1), dtype=coefficients.dtype)

i = np.arange(b.shape[0])

j = np.arange(b.shape[1])

b[np.not_equal.outer(i, j)] = coefficients.ravel()

import seaborn as sns
from sklearn.preprocessing import normalize
from sklearn import preprocessing

matrix=normalize(b, axis=1, norm='l2')

"""Heatmap of coefficients for the linear regression task. Everything is normalized with the l2 norm. This plot indicates how the variables are multicorreleted with each other """

ax = sns.heatmap(matrix,cmap='RdBu')
ax.figure.savefig('lin_heatmap.pdf', bbox_inches = 'tight')

"""Now I want to see for which variable we have the best regression. I look at the values of r2 and indentify the best with a bar chart."""

fig, ax = plt.subplots(figsize =(16, 9))
 
# Horizontal Bar Plot
ax.barh(col_names, coeff_r2)
 
# Remove axes splines
for s in ['top', 'bottom', 'left', 'right']:
    ax.spines[s].set_visible(False)
 
# Remove x, y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')
 
# Add padding between axes and labels
ax.xaxis.set_tick_params(pad = 5)
ax.yaxis.set_tick_params(pad = 10)
 
# Add x, y gridlines
ax.grid(b = True, color ='grey',
        linestyle ='-.', linewidth = 0.5,
        alpha = 0.2)
 
# Show top values
ax.invert_yaxis()
 
# Add annotation to bars
for i in ax.patches:
    plt.text(i.get_width()+0.02, i.get_y()+0.5,
             str(round((i.get_width()), 2)),
             fontsize = 10, fontweight ='bold',
             color ='black')
 
# Add Plot Title
ax.set_title('R2 of linear regression for every attribute',
             loc ='left', )
 
# Show Plot
plt.show()

ax.figure.savefig('r2_lin.pdf', bbox_inches = 'tight')

"""# Ridge

perform a linear regression over the target classes with the ridge penalty
"""

reg = Ridge()
reg.fit(X_train, y_train)
print('Coefficients: \n', reg.coef_)
print('Intercept: \n', reg.intercept_)

y_pred = reg.predict(X_test)
print('R2: %.3f' % r2_score(y_test, y_pred))
print('MSE: %.3f' % mean_squared_error(y_test, y_pred))
print('MAE: %.3f' % mean_absolute_error(y_test, y_pred))

"""do the same as you did for the linear regression, except that this time you have the ridge penalty."""

coefficients=np.zeros(shape=(0,22))

coeff_r2=[]

for element in col_names:
  dummy_x_tr=X_train_df.loc[:, X_train_df.columns != element]
  dummy_y_tr=X_train_df.loc[:, X_train_df.columns == element]
  dummy_x_te=X_test_df.loc[:, X_test_df.columns != element]
  dummy_y_te=X_test_df.loc[:, X_test_df.columns == element]
  X_test_dummy = dummy_x_te.values
  y_train_dummy = dummy_y_tr.values
  X_train_dummy = dummy_x_tr.values
  y_test_dummy = dummy_y_te.values
  reg = Ridge()
  reg.fit(X_train_dummy, y_train_dummy)

  dum_coef=reg.coef_

  
  coefficients = np.append(coefficients, dum_coef, axis=0)

  y_pred = reg.predict(X_test_dummy)

  coeff_r2.append(r2_score(y_test_dummy, y_pred))

b = np.zeros((coefficients.shape[0], coefficients.shape[1]+1), dtype=coefficients.dtype)

i = np.arange(b.shape[0])

j = np.arange(b.shape[1])

b[np.not_equal.outer(i, j)] = coefficients.ravel()

matrix=normalize(b, axis=1, norm='l2')

ax = sns.heatmap(matrix,cmap='RdBu')
ax.figure.savefig('ridge_heatmap.pdf', bbox_inches = 'tight')

fig, ax = plt.subplots(figsize =(16, 9))
 
# Horizontal Bar Plot
ax.barh(col_names, coeff_r2)
 
# Remove axes splines
for s in ['top', 'bottom', 'left', 'right']:
    ax.spines[s].set_visible(False)
 
# Remove x, y Ticks
ax.xaxis.set_ticks_position('none')
ax.yaxis.set_ticks_position('none')
 
# Add padding between axes and labels
ax.xaxis.set_tick_params(pad = 5)
ax.yaxis.set_tick_params(pad = 10)
 
# Add x, y gridlines
ax.grid(b = True, color ='grey',
        linestyle ='-.', linewidth = 0.5,
        alpha = 0.2)
 
# Show top values
ax.invert_yaxis()
 
# Add annotation to bars
for i in ax.patches:
    plt.text(i.get_width()+0.02, i.get_y()+0.5,
             str(round((i.get_width()), 2)),
             fontsize = 10, fontweight ='bold',
             color ='black')
 
# Add Plot Title
ax.set_title('R2 of linear regression for every attribute',
             loc ='left', )
 
# Show Plot
plt.show()

ax.figure.savefig('r2_lin.pdf', bbox_inches = 'tight')

"""# Lasso

Select the best, worse and middle performing variables according to lasso regularization, then do a grid search to identify the best paramenters for alpha (tuning of hyperparameter)
"""

variabili=['tGravityAcc-mean()-X','tBodyAcc-correlation()-X,Y','fBodyGyro-maxInds-X']


r2_to_plot=[]
for element in variabili:
  dummy_x_tr=X_train_df.loc[:, X_train_df.columns != element]
  dummy_y_tr=X_train_df[[element]]
  dummy_x_te=X_test_df.loc[:, X_test_df.columns != element]
  dummy_y_te=X_test_df[[element]]
  X_test_dummy = dummy_x_te.values
  y_train_dummy = dummy_y_tr.values
  X_train_dummy = dummy_x_tr.values
  y_test_dummy = dummy_y_te.values

  alpha=[1,0.1,0.01,0.001,0.0001,0.00001,0.000001,0.0000001]
  r2_scores=[]
  for al in alpha:
    reg = Lasso(alpha=al)
    reg.fit(X_train_dummy, y_train_dummy)
    y_pred = reg.predict(X_test_dummy)
    r2_scores.append(r2_score(y_test_dummy, y_pred))
    #print('Multi regression for class ', element)
    #print(al, 'R2: %.3f' % r2_score(y_test_dummy, y_pred))
    #print(r2_scores)
  r2_to_plot.append(r2_scores)
  #print('Multi regression for class ', 'tGravityAcc-mean()-X')
  #print('Coefficients: \n', reg.coef_)
  #print('Intercept: \n', reg.intercept_)
  #print('R2: %.3f' % r2_score(y_test_dummy, y_pred))
  #print('MSE: %.3f' % mean_squared_error(y_test_dummy, y_pred))
  #print('MAE: %.3f' % mean_absolute_error(y_test_dummy, y_pred))

fig, ax =plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("R2")
ax.set_title("R2 vs alpha for various classes")
ax.plot(alpha,r2_to_plot[0], marker ='.',label=variabili[0],drawstyle='steps-post')
ax.plot(alpha,r2_to_plot[1], marker ='.',label=variabili[1],drawstyle='steps-post')
ax.plot(alpha,r2_to_plot[2], marker ='.',label=variabili[2],drawstyle='steps-post')
ax.set_xscale('log')
ax.legend()
ax.figure.savefig('alpha_tuning.pdf', bbox_inches = 'tight')

plt.show()

for element in variabili:
  dummy_x_tr=X_train_df.loc[:, X_train_df.columns != element]
  dummy_y_tr=X_train_df[[element]]
  dummy_x_te=X_test_df.loc[:, X_test_df.columns != element]
  dummy_y_te=X_test_df[[element]]
  X_test_dummy = dummy_x_te.values
  y_train_dummy = dummy_y_tr.values
  X_train_dummy = dummy_x_tr.values
  y_test_dummy = dummy_y_te.values

  reg = Lasso(alpha=0.001)
  reg.fit(X_train_dummy, y_train_dummy)
  y_pred = reg.predict(X_test_dummy)
  print('Multi regression for class ', element)
  print('Coefficients: \n', reg.coef_)
  print('Intercept: \n', reg.intercept_)
  print('R2: %.3f' % r2_score(y_test_dummy, y_pred))
  #print('MSE: %.3f' % mean_squared_error(y_test_dummy, y_pred))
  #print('MAE: %.3f' % mean_absolute_error(y_test_dummy, y_pred))

for element in variabili:
  dummy_x_tr=X_train_df.loc[:, X_train_df.columns != element]
  dummy_y_tr=X_train_df[[element]]
  dummy_x_te=X_test_df.loc[:, X_test_df.columns != element]
  dummy_y_te=X_test_df[[element]]
  X_test_dummy = dummy_x_te.values
  y_train_dummy = dummy_y_tr.values
  X_train_dummy = dummy_x_tr.values
  y_test_dummy = dummy_y_te.values

  reg = Ridge()
  reg.fit(X_train_dummy, y_train_dummy)
  y_pred = reg.predict(X_test_dummy)
  print('Multi regression for class ', element)
  print('Coefficients: \n', reg.coef_)
  print('Intercept: \n', reg.intercept_)
  print('R2: %.3f' % r2_score(y_test_dummy, y_pred))
  #print('MSE: %.3f' % mean_squared_error(y_test_dummy, y_pred))
  #print('MAE: %.3f' % mean_absolute_error(y_test_dummy, y_pred))

"""# Logistic Regression

Faccio due classi: dynamic (1,2,3 aka walking, upstairs and downstairs) e static (4,5,6 aka sitting, standing laying)
"""

y_train_bi=[]
for element in y_train:
  if element<4:
    new_el=[0]
  else:
    new_el=[1]
  y_train_bi.append(new_el)

y_test_bi=[]
for element in y_test:
  if element<4:
    new_el=[0]
  else:
    new_el=[1]
  y_test_bi.append(new_el)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

from scipy.special import expit

"""Per ogni attributo mi vado a vedere la logistic regression"""

target_names=['WALKING','WALKING UPSTAIRS','WALKING DOWNSTAIRS','SITTING','STANDING']

clf = LogisticRegression(random_state=0,multi_class='multinomial')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print('Accuracy %s' % accuracy_score(y_test, y_pred))
print('F1-score %s' % f1_score(y_test, y_pred, average='weighted'))
report=classification_report(y_test, y_pred,output_dict=True,digits=2,target_names=target_names)
report_df = pd.DataFrame(report).transpose()

print(report_df.to_latex())

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(clf,
                      X_test,
                      y_test,
                      cmap='Blues',
                      values_format='d',
                      colorbar=False
                      )
plt.savefig('confusion_matrix.pdf',bbox_inches = 'tight')

def plot_multiclass_roc(clf, X, y, clf_name, ax = None):
    if not isinstance(y, type(np.array([]))):
        y = y.values
    if ax is None:
        fig = plt.figure(figsize = (6, 4))
        ax = fig.add_subplot()
    y_pred_proba = clf.predict_proba(X)   
    for i, label in enumerate(np.unique(y)):
        class_i_arr = (y == label).astype(int)
        class_i_prob_arr = y_pred_proba[:, i]
        RocCurveDisplay.from_predictions(class_i_arr, class_i_prob_arr,
                        name = clf_name + f' of class {label}', ax = ax)
    random = ax.plot(np.linspace(0,1,100), np.linspace(0,1,100), 'k--', 
                 label = 'Random Classifier')
    ax.set_ylabel('True Positive Rate', fontsize = 14)
    ax.set_xlabel('False Positive Rate', fontsize = 14)

fig, ax = plt.subplots(1,1, figsize = (6, 4))
plot_multiclass_roc(clf, X_test, y_test, 
                    'Log Reg', ax = ax)
ax.legend(fontsize = 'small')
plt.savefig('log_multiclass_roc.pdf', bbox_inches = 'tight')
plt.show()

"""poi vado a vedere come contribuiscono singolarmente le variabili in sta multi"""

score_array=[]

for i in range(0,23):
  print("ITERATION %d",i)
  clf = LogisticRegression(random_state=0)
  clf.fit(X_train.T[i].reshape(-1,1), y_train_bi)
  y_pred = clf.predict(X_test.T[i].reshape(-1,1))

  print('Accuracy %s' % accuracy_score(y_test_bi, y_pred))
  print('F1-score %s' % f1_score(y_test_bi, y_pred, average='weighted'))
  print(classification_report(y_test_bi, y_pred))

  score_array.append([i,accuracy_score(y_test_bi, y_pred),f1_score(y_test_bi, y_pred, average='weighted')])

  loss = expit(sorted(X_test.T[i].reshape(-1,1)) * clf.coef_ + clf.intercept_).ravel()
  plt.plot(sorted(X_test.T[i].reshape(-1,1)), loss, color='red', linewidth=3)
  plt.scatter(X_train.T[i].reshape(-1,1), y_train_bi)
  #plt.xlabel(selected_columns[0], fontsize=16)
  #plt.ylabel('Occupancy', fontsize=16)
  plt.tick_params(axis='both', which='major', labelsize=16)
  plt.show()

"""Stampami: numero variabile, accuracy e f1 score"""

np.round(score_array,2)

"""Altro approccio: guarda le variabili a due a due (tipo classi 1 e 2, poi 1 e 3, etc...)."""

from numpy.lib.function_base import kaiser
y_train_couple=[]

X_train_couple=[]
list_names=[]

for j in range(1,5):
  for k in range(j+1,6):
    y_dummy=[]
    X_dummy=[]
    list_names.append([j,k])
    for i in range(0,len(y_train)):
      clas=y_train[i][0]
      if clas==j or clas==k:
        y_dummy.append(clas)
        X_dummy.append((X_train[i]))

    y_train_couple.append(y_dummy)
    X_train_couple.append(X_dummy)

list_names

y_test_couple=[]

X_test_couple=[]


for j in range(1,5):
  for k in range(j+1,6):
    y_dummy=[]
    X_dummy=[]
    for i in range(0,len(y_test)):
      clas=y_test[i][0]
      if clas==j or clas==k:
        y_dummy.append(clas)
        X_dummy.append(list(X_test[i]))

    y_test_couple.append(y_dummy)
    X_test_couple.append(X_dummy)

len(X_train_couple)

names_couple=['1-2','1-3','1-4','1-5','2-3','2-4','2-5','3-4','3-5','4-5']

"""e qua riperformo la multi che ho fatt osopra. tanto ho capito che quell'altra è inutile."""

from sklearn.metrics import roc_curve, auc, roc_auc_score

fig1 = plt.figure(figsize = (6, 4))
fig2 = plt.figure(figsize = (6, 4))


ax1 = fig1.add_subplot()

ax2 = fig2.add_subplot()

for (x_tr,y_tr,x_te,y_te,ele,i) in zip(X_train_couple,y_train_couple,X_test_couple,y_test_couple,list_names,names_couple):
  clf = LogisticRegression(random_state=0,multi_class='auto')
  clf.fit(x_tr, y_tr)
  y_pred = clf.predict(x_te)
  print('Classification for variables ',i)
  print('Accuracy %s' % accuracy_score(y_te, y_pred))
  print('F1-score %s' % f1_score(y_te, y_pred, average='weighted'))
  print(classification_report(y_te, y_pred))

  y_score = clf.predict_proba(x_te)
  y_te_dummy=[]
  for ite in y_te: 
    if(ite==ele[0]):
      ite=0
    if(ite==ele[1]):
      ite=1
    y_te_dummy.append(ite)
  fpr_el, tpr_el, th = roc_curve(y_te_dummy, y_score[:,1])

  roc_auc_el = auc(fpr_el, tpr_el)

  if(X_train_couple.index(x_tr)<5):
    ax1.plot(fpr_el, tpr_el, label='$AUC$ %s = %.3f' % (i,roc_auc_el))
  if(X_train_couple.index(x_tr)>4):
    ax2.plot(fpr_el, tpr_el, label='$AUC$ %s = %.3f' % (i,roc_auc_el))

ax1.legend(loc="lower right", fontsize=8, frameon=False)
ax1.plot([0,1], [0,1], 'k--')
ax1.set_xlabel('False Positive Rate', fontsize=20)
ax1.set_ylabel('True Positive Rate', fontsize=20)
ax1.legend(fontsize = 'small')
ax1.legend(loc="lower right", fontsize=8, frameon=False)
ax2.plot([0,1], [0,1], 'k--')
ax2.set_xlabel('False Positive Rate', fontsize=20)
ax2.set_ylabel('True Positive Rate', fontsize=20)
ax2.legend(fontsize = 'small')

ax1.figure.savefig('log_roc_1.pdf', bbox_inches = 'tight')
ax2.figure.savefig('log_roc_2.pdf', bbox_inches = 'tight')


#plt.tick_params(axis='both', which='major', labelsize=22)
plt.show()

"""se si analizzano a mano le cose si impazzisce. disegno la roc e via."""

master_score_array=[]
for (x_tr,y_tr,x_te,y_te,j,ele) in zip(X_train_couple,y_train_couple,X_test_couple,y_test_couple,range(0,15),list_names):
  score_array=[]
  for i in range(0,23):
    clf = LogisticRegression(random_state=0)
    clf.fit(np.array(x_tr).T[i].reshape(-1,1), y_tr)
    y_pred = clf.predict(np.array(x_te).T[i].reshape(-1,1))

    #print('Accuracy %s' % accuracy_score(y_test_bi, y_pred))
    #print('F1-score %s' % f1_score(y_test_bi, y_pred, average='weighted'))
    #print(classification_report(y_test_bi, y_pred))

    score_array.append([i,np.round(accuracy_score(y_te, y_pred),2),np.round(f1_score(y_te, y_pred, average='weighted'),2)])

    #loss = expit(sorted(X_test.T[i].reshape(-1,1)) * clf.coef_ + clf.intercept_).ravel()
    #plt.plot(sorted(X_test.T[i].reshape(-1,1)), loss, color='red', linewidth=3)
    #plt.scatter(X_train.T[i].reshape(-1,1), y_train_bi)
    #plt.xlabel(selected_columns[0], fontsize=16)
    #plt.ylabel('Occupancy', fontsize=16)
    #plt.tick_params(axis='both', which='major', labelsize=16)
    #plt.show()
  master_score_array.append(ele)
  master_score_array.append(score_array)

master_score_array