# -*- coding: utf-8 -*-
"""10_ensemble.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1egSLtsTJ6OyxRaXWM5WGLSx_zts8wbw4

**Author:** Giulio Cordova

**Python version:**  3.x
"""



# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from collections import defaultdict

from sklearn.model_selection import train_test_split, cross_val_score , GridSearchCV

from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix
from sklearn.metrics import roc_curve, auc, roc_auc_score, plot_confusion_matrix,RocCurveDisplay

import warnings
warnings.filterwarnings('ignore')
warnings.filterwarnings(action='once')

"""# Data Preparation"""

X_test_df = pd.read_csv('best_X_test_con_nomi.csv', header = 0)
y_train_df = pd.read_csv('y_train_wo_outliers_num.csv', header = None)
X_train_df = pd.read_csv('best_X_train_con_nomi.csv', header = 0)
y_test_df = pd.read_fwf('y_test.txt', header = None)

X_test = X_test_df.values
y_train = y_train_df.values
X_train = X_train_df.values
y_test = y_test_df.values

X_train_n=np.zeros(shape=[0,23])
y_train_n=np.zeros(shape=[0,1])
for j in range(0,7274):
  if y_train[j]!=6:

    X_train_n = np.append(X_train_n, [X_train[j]], axis=0)
    y_train_n = np.append(y_train_n, [y_train[j]], axis=0)

X_test_n=np.zeros(shape=[0,23])
y_test_n=np.zeros(shape=[0,1])
for j in range(0,len(y_test)):
  if y_test[j]!=6:

    X_test_n = np.append(X_test_n, [X_test[j]], axis=0)
    y_test_n = np.append(y_test_n, [y_test[j]], axis=0)

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

from sklearn.inspection import permutation_importance

col_names=list(X_test_df.columns)

import pydotplus
from sklearn import tree
from IPython.display import Image

from sklearn.model_selection import cross_val_score

X=np.concatenate((X_train_n,X_test_n))
y=np.concatenate((y_train_n,y_test_n))

"""### Tuning the hyper-parameters"""

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

param_list = {'max_depth': [None,2,5,10,15,20],
              'min_samples_split': [2, 5, 10, 20,50,100],
              'min_samples_leaf': [1, 5, 10, 20,50,100],
              'n_estimators': [50,100,200]
              }
clf=RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid=param_list, cv=5)

print(grid_search.get_params)

grid_search.fit(X_train_n, y_train_n)
clf = grid_search.best_estimator_

y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average='weighted'))
print(classification_report(y_test_n, y_pred))

grid_search.cv_results_['params'][grid_search.cv_results_['rank_test_score'][0]]

acc = []
n_estimators=[10,50,80,100,200,300,400,500,600,700,800,900,1000]
# Will take some time
from sklearn import metrics
for element in n_estimators:
    clf = RandomForestClassifier(random_state=42,max_depth=None, min_samples_leaf=10, 
                                 min_samples_split=5,n_estimators=element)
    clf.fit(X_train_n, y_train_n)
    y_pred = clf.predict(X_test_n)
    acc.append(accuracy_score(y_test_n, y_pred))
    
plt.figure(figsize=(6,4))
plt.plot(n_estimators,acc,linestyle='dashed', 
         marker='.',markersize=10)
#plt.title('accuracy vs. #estimators')
plt.xlabel('#estimators')
plt.ylabel('Accuracy')
plt.savefig('RF_accuracy.pdf', bbox_inches = 'tight')
print("Maximum accuracy:-",max(acc),"at K =",acc.index(max(acc)))

clf = RandomForestClassifier(random_state=42,max_depth=None, min_samples_leaf=10, 
                                 min_samples_split=5,n_estimators=200)

clf.fit(X_train_n, y_train_n)
y_pred = clf.predict(X_test_n)

acc = []
max_samples=[0.00001,0.0001,0.001,0.01,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
# Will take some time
from sklearn import metrics
for element in max_samples:
    clf = RandomForestClassifier(random_state=42,max_depth=None, min_samples_leaf=10, 
                                 min_samples_split=5,n_estimators=200,max_samples=element)
    clf.fit(X_train_n, y_train_n)
    y_pred = clf.predict(X_test_n)
    acc.append(accuracy_score(y_test_n, y_pred))
    
plt.figure(figsize=(6,4))
plt.plot(max_samples,acc,linestyle='dashed', 
         marker='.', markersize=10)
#plt.title('accuracy vs. #estimators')
plt.xlabel('Max Samples in Bootstrap')
plt.ylabel('Accuracy')
plt.xscale('log')
plt.savefig('RF_maxsamples.pdf', bbox_inches = 'tight')
print("Maximum accuracy:-",max(acc),"at K =",acc.index(max(acc)))

"""##Cross Validation"""

#clf = RandomForestClassifier()
scores = cross_val_score(clf, X, y, cv=5)

print('Accuracy %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))

"""##Feature importance"""

nbr_features = 7

tree_feature_importances = clf.feature_importances_
sorted_idx = tree_feature_importances.argsort()[-nbr_features:]

y_ticks = np.arange(0, len(sorted_idx))
fig, ax = plt.subplots()
plt.barh(y_ticks, tree_feature_importances[sorted_idx])
plt.yticks(y_ticks, np.array(list(X_test_df.columns))[sorted_idx])
plt.title("Random Forest Feature Importances (MDI)")
plt.tight_layout()
plt.savefig('RF_importance.pdf', bbox_inches = 'tight')

plt.show()

result = permutation_importance(clf, X_test_n, y_test_n, n_repeats=10, random_state=42, n_jobs=2)

sorted_idx = result.importances_mean.argsort()[-nbr_features:]

fig, ax = plt.subplots()
plt.boxplot(result.importances[sorted_idx].T, vert=False, labels=np.array(col_names)[sorted_idx])
plt.title("Permutation Importances (test set)")
plt.tight_layout()
plt.savefig('random_importance.pdf', bbox_inches = 'tight')
plt.show()

"""##ROC, confusion matrix, etc."""

target_names=['WALKING','WALKING UPSTAIRS','WALKING DOWNSTAIRS','SITTING','STANDING']

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average='weighted'))
report=classification_report(y_test_n, y_pred,output_dict=True,digits=2,target_names=target_names)
report_df = pd.DataFrame(report).transpose()

print(report_df.to_latex())

from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(clf,
                      X_test_n,
                      y_test_n,
                      cmap='Blues',
                      values_format='d',
                      colorbar=False
                      )
plt.savefig('confusion_matrix_random_forest.pdf',bbox_inches = 'tight')

def plot_multiclass_roc(clf, X, y, clf_name, ax = None):
    if not isinstance(y, type(np.array([]))):
        y = y.values
    if ax is None:
        fig = plt.figure(figsize = (6, 4))
        ax = fig.add_subplot()
    y_pred_proba = clf.predict_proba(X)   
    for i, label in enumerate(np.unique(y)):
        class_i_arr = (y == label).astype(int)
        class_i_prob_arr = y_pred_proba[:, i]
        RocCurveDisplay.from_predictions(class_i_arr, class_i_prob_arr,
                        name = clf_name + f' of class {label}', ax = ax)
    random = ax.plot(np.linspace(0,1,100), np.linspace(0,1,100), 'k--', 
                 label = 'Random Classifier')
    ax.set_ylabel('True Positive Rate', fontsize = 14)
    ax.set_xlabel('False Positive Rate', fontsize = 14)

fig, ax = plt.subplots(1,1, figsize = (6, 4))
plot_multiclass_roc(clf, X_test_n, y_test_n, 
                    'Random Forest', ax = ax)
ax.legend(fontsize = 'small')
plt.savefig('RF_multiclass_roc.pdf', bbox_inches = 'tight')
plt.show()

"""##Random Search"""

param_list = {'max_depth': [None] + list(np.arange(2, 20)),
              'min_samples_split': [2, 5, 10, 20, 30, 50, 100],
              'min_samples_leaf': [1, 5, 10, 20, 30, 50, 100],
              'n_estimators': [50,100,200,500]
              #'min_weight_fraction_leaf':list(np.arange(0,0.5,0.1)),
              #'max_features': ['log2','sqrt',None]
             }


warnings.filterwarnings(action='once')
random_search = RandomizedSearchCV(clf, param_distributions=param_list, 
                                   n_iter=50, cv=5,random_state=42)
random_search.fit(X_train_n, y_train_n)
clf = random_search.best_estimator_


y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average='weighted'))
print(classification_report(y_test_n, y_pred))

random_search.cv_results_['params'][random_search.cv_results_['rank_test_score'][0]]

"""###Out of bag error"""

import matplotlib.pyplot as plt

from collections import OrderedDict
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier


RANDOM_STATE = 42

ensemble_clfs = [
    (
        "RandomForestClassifier, max_features='sqrt'",
        RandomForestClassifier(
            warm_start=True,
            oob_score=True,
            max_features="sqrt",
            random_state=RANDOM_STATE,
        ),
    ),
    (
        "RandomForestClassifier, max_features='log2'",
        RandomForestClassifier(
            warm_start=True,
            max_features="log2",
            oob_score=True,
            random_state=RANDOM_STATE,
        ),
    ),
    (
        "RandomForestClassifier, max_features=None",
        RandomForestClassifier(
            warm_start=True,
            max_features=None,
            oob_score=True,
            random_state=RANDOM_STATE,
        ),
    ),
]

# Map a classifier name to a list of (<n_estimators>, <error rate>) pairs.
error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)

# Range of `n_estimators` values to explore.
min_estimators = 100
max_estimators = 1000

for label, clf in ensemble_clfs:
    for i in range(min_estimators, max_estimators + 1, 5):
        clf.set_params(n_estimators=i)
        clf.fit(X, y)

        # Record the OOB error for each `n_estimators=i` setting.
        oob_error = 1 - clf.oob_score_
        error_rate[label].append((i, oob_error))

# Generate the "OOB error rate" vs. "n_estimators" plot.
for label, clf_err in error_rate.items():
    xs, ys = zip(*clf_err)
    plt.plot(xs, ys, label=label)

plt.xlim(min_estimators, max_estimators)
plt.xlabel("n_estimators")
plt.ylabel("OOB error rate")
plt.legend(loc="upper right")
plt.show()

"""# Bagging"""

from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier

"""If None, then the base estimator is a decision tree."""

param_list = {'max_depth': [None,2,5,10,15,20],
              'min_samples_split': [2, 5, 10, 20,50,100],
              'min_samples_leaf': [1, 5, 10, 20,50,100],
              'n_estimators': [50,100,200]
              }
clf=BaggingClassifier(random_state=42)
grid_search = GridSearchCV(clf, param_grid=param_list, cv=5)

print(grid_search.get_params)

grid_search.fit(X_train_n, y_train_n)
clf = grid_search.best_estimator_

y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average='weighted'))
print(classification_report(y_test_n, y_pred))

clf = BaggingClassifier(base_estimator=None, n_estimators=100, random_state=0)
clf.fit(X_train_n, y_train_n)

y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average=None))
print(classification_report(y_test_n, y_pred))

clf = BaggingClassifier(base_estimator=SVC(C=1000), n_estimators=10, random_state=0)

clf.fit(X_train_n, y_train_n)

y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average=None))
print(classification_report(y_test_n, y_pred))

clf = BaggingClassifier(base_estimator=RandomForestClassifier(n_estimators=100), n_estimators=100, random_state=0)
clf.fit(X_train_n, y_train_n)

y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average=None))
print(classification_report(y_test_n, y_pred))

clf=BaggingClassifier()

param_list = {'base_estimator': [RandomForestClassifier(),SVC(),None],
              'n_estimators': [10,50,100,500,1000]
             }


warnings.filterwarnings(action='once')
random_search = RandomizedSearchCV(clf, param_distributions=param_list, n_iter=20, cv=5)
random_search.fit(X_train_n, y_train_n)
clf = random_search.best_estimator_


y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average='weighted'))
print(classification_report(y_test_n, y_pred))

"""# Boosting"""

from sklearn.ensemble import AdaBoostClassifier

"""If None, then the base estimator is DecisionTreeClassifier(max_depth=1)."""

# example of grid searching key hyperparameters for adaboost on a classification dataset
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier
# define the model with default hyperparameters
model = AdaBoostClassifier()
# define the grid of values to search
grid = dict()
grid['n_estimators'] = [10, 30,50,75, 100]
grid['learning_rate'] = [0.001, 0.01, 0.01, 0.1, 1.0]
# define the evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)
# define the grid search procedure
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')
# execute the grid search
grid_result = grid_search.fit(X, y)
# summarize the best score and configuration
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
# summarize all scores that were evaluated
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

"""visualisation of best number of estimators"""

clf = AdaBoostClassifier(base_estimator=None, n_estimators=30, random_state=1,learning_rate=0.1)
clf.fit(X_train_n, y_train_n)

y_pred = clf.predict(X_test_n)

print('Accuracy %s' % accuracy_score(y_test_n, y_pred))
print('F1-score %s' % f1_score(y_test_n, y_pred, average='weighted'))
report=classification_report(y_test_n, y_pred,output_dict=True,digits=2,target_names=target_names)
report_df = pd.DataFrame(report).transpose()

print(report_df.to_latex())

acc = []
n_estimators=[10,30,50,80,100,200,300,400,500,600,700,800,900,1000]
# Will take some time
from sklearn import metrics
for element in n_estimators:
    clf = AdaBoostClassifier(random_state=1,learning_rate=0.1,n_estimators=element)
    clf.fit(X_train_n, y_train_n)
    y_pred = clf.predict(X_test_n)
    acc.append(accuracy_score(y_test_n, y_pred))
    
plt.figure(figsize=(6,4))
plt.plot(n_estimators,acc,linestyle='dashed', 
         marker='.', markersize=10)
#plt.title('accuracy vs. #estimators')
plt.xlabel('#estimators')
plt.ylabel('Accuracy')
plt.savefig('Ada_accuracy.pdf', bbox_inches = 'tight')
print("Maximum accuracy:-",max(acc),"at K =",acc.index(max(acc)))

"""Max samples"""

acc = []
max_samples=[0.00001,0.0001,0.001,0.01,0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
# Will take some time
from sklearn import metrics
for element in max_samples:
    clf = AdaBoostClassifier(base_estimator=None, n_estimators=30,learning_rate=0.1, random_state=1)
    clf.fit(X_train_n, y_train_n)
    y_pred = clf.predict(X_test_n)
    acc.append(accuracy_score(y_test_n, y_pred))
    
plt.figure(figsize=(6,4))
plt.plot(max_samples,acc,linestyle='dashed', 
         marker='.', markersize=10)
#plt.title('accuracy vs. #estimators')
plt.xlabel('Max Samples in Bootstrap')
plt.ylabel('Accuracy')
plt.xscale('log')
plt.savefig('Ada_maxsamples.pdf', bbox_inches = 'tight')
print("Maximum accuracy:-",max(acc),"at K =",acc.index(max(acc)))